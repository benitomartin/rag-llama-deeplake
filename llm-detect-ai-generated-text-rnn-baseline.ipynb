{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"padding:20px; \n",
    "            color:#150d0a;\n",
    "            margin:10px;\n",
    "            font-size:220%;\n",
    "            text-align:center;\n",
    "            display:fill;\n",
    "            border-radius:20px;\n",
    "            border-width: 5px;\n",
    "            border-style: solid;\n",
    "            border-color: #150d0a;\n",
    "            background-color:#4FC95F;\n",
    "            overflow:hidden;\n",
    "            font-weight:500\">LLM - Detect AI Generated Text</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4](https://github.com/benitomartin/benitomartin/assets/116911431/cab5bb0e-1473-47f5-8e9c-d51e1731b207)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <div style=\"padding:20px; \n",
    "              color:blue;\n",
    "              margin:10px;\n",
    "              font-size:150%;\n",
    "              text-align:center;\n",
    "              display:fill;\n",
    "              border-radius:20px;\n",
    "              border-width: 5px;\n",
    "              background-color:#eca912;\n",
    "              overflow:hidden;\n",
    "              font-weight:500\">\n",
    "    <b>INTRODUCTION</b>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has been created as part of the **LLM - Detect AI Generated Text** competition from **Kaggle**.\n",
    "\n",
    "The competition dataset comprises about 10,000 essays, some written by students and some generated by a variety of large language models (LLMs). The goal of the competition is to determine whether or not essay was generated by an LLM.\n",
    "\n",
    "All of the essays were written in response to one of seven essay prompts. In each prompt, the students were instructed to read one or more source texts and then write a response. This same information may or may not have been provided as input to an LLM when generating an essay.\n",
    "\n",
    "Essays from two of the prompts compose the training set; the remaining essays compose the hidden test set. Nearly all of the training set essays were written by students, with only a few generated essays given as examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATASET**\n",
    "**test|train_essays.csv** \n",
    "\n",
    "- `id`- A unique identifier for each essay.\n",
    "\n",
    "- `prompt_id` - Identifies the prompt the essay was written in response to.\n",
    "\n",
    "- `text` - The essay text itself.\n",
    "\n",
    "- `generated` - Whether the essay was written by a student (`0`) or generated by an LLM (`1`). This field is the target and is not present in `test_essays.csv`.\n",
    "\n",
    "**train_prompts.csv** - Essays were written in response to information in these fields.\n",
    "\n",
    "- `prompt_id` - A unique identifier for each prompt.\n",
    "\n",
    "- `prompt_name` - The title of the prompt.\n",
    "instructions - The instructions given to students.\n",
    "\n",
    "- `source_text` - The text of the article(s) the essays were written in response to, in Markdown format. Significant paragraphs are enumerated by a numeral preceding the paragraph on the same line, as in `0 Paragraph one.\\n\\n1 Paragraph two`.. Essays sometimes refer to a paragraph by its numeral. Each article is preceded with its title in a heading, like `# Title`. When an author is indicated, their name will be given in the title after `by`. Not all articles have authors indicated. An article may have subheadings indicated like `## Subheading`.\n",
    "\n",
    "**sample_submission.csv** - A submission file in the correct format. See the Evaluation page for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <div style=\"padding:20px; \n",
    "              color:blue;\n",
    "              margin:10px;\n",
    "              font-size:150%;\n",
    "              text-align:center;\n",
    "              display:fill;\n",
    "              border-radius:20px;\n",
    "              border-width: 5px;\n",
    "              background-color:#eca912;\n",
    "              overflow:hidden;\n",
    "              font-weight:500\">\n",
    "    <b>CONTACT INFORMATION</b>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like this notebook, feel free to upvote it and connect with me!\n",
    "\n",
    "**Benito Martin:** \n",
    "\n",
    "- [LinkedIn](https://www.linkedin.com/in/benitomzh/) 🔗\n",
    "\n",
    "- [GitHub](https://github.com/benitomartin) 🔗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <div style=\"padding:20px; \n",
    "              color:blue;\n",
    "              margin:10px;\n",
    "              font-size:150%;\n",
    "              text-align:center;\n",
    "              display:fill;\n",
    "              border-radius:20px;\n",
    "              border-width: 5px;\n",
    "              background-color:#eca912;\n",
    "              overflow:hidden;\n",
    "              font-weight:500\">\n",
    "    <b>TABLE OF CONTENTS</b>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1. Import Libraries](#libraries)\n",
    "\n",
    "* [2. Helper Functions](#helper)\n",
    "\n",
    "* [3. Import Data](#data)\n",
    "    \n",
    "    * [3.1 Essays Datasets](#essays)\n",
    "        \n",
    "        * [3.1.1. Train Set](#essaystrain)   \n",
    "    \n",
    "        * [3.1.1. Test Set](#essaystest) \n",
    "    \n",
    "    * [3.2. Prompts Dataset](#prompts)\n",
    "\n",
    "* [4. Exploratory Data Analysis](#eda)\n",
    "    \n",
    "    * [4.1. Distribution](#distribution)\n",
    "    \n",
    "        * [4.1.1. Essays](#distess)\n",
    "            \n",
    "* [5. External Essays](#distext)\n",
    "    \n",
    "    * [5.1. DAIGT Essay](#daigt)\n",
    "    \n",
    "    * [5.2. Distribution](#dist)\n",
    "\n",
    "    * [5.3. Wordcloud](#wc)\n",
    "    \n",
    "    * [5.4. Preprocessing](#prepro)\n",
    "        \n",
    "* [6. Modeling](#modeling)\n",
    "\n",
    "    * [6.1. Train/Test Split](#split)\n",
    "\n",
    "    * [6.2. Vectorization](#vector)\n",
    "    \n",
    "    * [6.3. Embedding](#emb)\n",
    "    \n",
    "    * [6.4. RNN Model](#model)\n",
    "  \n",
    "         * [6.4.1. Fitting](#fit)\n",
    "         \n",
    "         * [6.4.2. Learning Curves](#lc) \n",
    "         \n",
    "         * [6.4.3. Confusion Matrix](#cm) \n",
    "         \n",
    "         * [6.4.4. Classification Report](#cr)\n",
    "        \n",
    "         * [6.4.5. AUC - ROC Curve](#roc)\n",
    "\n",
    "         * [6.4.6. Precission-Recall Curve](#prc)\n",
    "\n",
    "* [7. Submission](#sub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='289C4E'>1. Import Libraries 📚<font><a class='anchor' id='libraries'></a> [↑](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:27.496320Z",
     "iopub.status.busy": "2024-01-04T20:03:27.495903Z",
     "iopub.status.idle": "2024-01-04T20:03:46.968275Z",
     "shell.execute_reply": "2024-01-04T20:03:46.966872Z",
     "shell.execute_reply.started": "2024-01-04T20:03:27.496291Z"
    }
   },
   "outputs": [],
   "source": [
    "# LIBRARIES\n",
    "\n",
    "# Global\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import regex as re\n",
    "\n",
    "# Function to plot WordCloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from collections import Counter\n",
    "\n",
    "# Tensorflow/Keras\n",
    "import tensorflow as tf\n",
    "import keras_core as keras\n",
    "from keras import layers, Sequential\n",
    "from keras.layers import TextVectorization\n",
    "from keras.callbacks import (ModelCheckpoint, \n",
    "                             EarlyStopping, \n",
    "                             ReduceLROnPlateau, \n",
    "                             CSVLogger, \n",
    "                             LearningRateScheduler)\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay, \n",
    "                             classification_report, precision_recall_curve, \n",
    "                             roc_curve, auc) \n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # or \"tensorflow\" or \"torch\"\n",
    "\n",
    "# Set Seed for Reproducibility\n",
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "# Use mixed precision to speed up all training.\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# Check Versions\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"Keras:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='289C4E'>2. Helper Functions 💾<font><a class='anchor' id='helper'></a> [↑](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition provides an **Efficiency Prize**, which will only be awarded on CPU Only notebooks. Therefore in order to reduce the size of the imported dataframes, the following function will be used to to **downcast** numerical columns to more memory-efficient types, hence improving its memory usage.\n",
    "\n",
    "\n",
    "Additionally a function to plot **WorClouds** is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:46.971114Z",
     "iopub.status.busy": "2024-01-04T20:03:46.970431Z",
     "iopub.status.idle": "2024-01-04T20:03:46.988707Z",
     "shell.execute_reply": "2024-01-04T20:03:46.987646Z",
     "shell.execute_reply.started": "2024-01-04T20:03:46.971079Z"
    }
   },
   "outputs": [],
   "source": [
    "def compress(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Reduces the size of the DataFrame by downcasting numerical columns\n",
    "    \"\"\"\n",
    "    input_size = df.memory_usage(index=True).sum() / (1024 ** 2)\n",
    "    if verbose:\n",
    "        print(\"Old dataframe size:\", round(input_size, 2), 'MB')\n",
    "\n",
    "    in_size = df.memory_usage(index=True).sum()\n",
    "    dtype_before = df.dtypes.copy()  # Copy of original data types\n",
    "\n",
    "    for col in df.select_dtypes(include=['float64', 'int64']):\n",
    "        col_type = df[col].dtype\n",
    "        col_min, col_max = df[col].min(), df[col].max()\n",
    "\n",
    "        if col_type == 'int64':\n",
    "            if col_min > np.iinfo(np.int8).min and col_max < np.iinfo(np.int8).max:\n",
    "                df[col] = df[col].astype(np.int8)\n",
    "            elif col_min > np.iinfo(np.int16).min and col_max < np.iinfo(np.int16).max:\n",
    "                df[col] = df[col].astype(np.int16)\n",
    "            elif col_min > np.iinfo(np.int32).min and col_max < np.iinfo(np.int32).max:\n",
    "                df[col] = df[col].astype(np.int32)\n",
    "            elif col_min > np.iinfo(np.int64).min and col_max < np.iinfo(np.int64).max:\n",
    "                df[col] = df[col].astype(np.int64)\n",
    "        elif col_type == 'float64':\n",
    "            ## float16 warns of overflow\n",
    "            # if col_min > np.finfo(np.float16).min and col_max < np.finfo(np.float16).max:\n",
    "            #     df[col] = df[col].astype(np.float16)\n",
    "            if col_min > np.finfo(np.float32).min and col_max < np.finfo(np.float32).max:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            elif col_min > np.finfo(np.float64).min and col_max < np.finfo(np.float64).max:\n",
    "                df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    out_size = df.memory_usage(index=True).sum()\n",
    "    ratio = (1 - round(out_size / in_size, 2)) * 100\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Optimized size by {}%\".format(round(ratio, 2)))\n",
    "        print(\"New DataFrame size:\", round(out_size / (1024 ** 2), 2), \"MB\")\n",
    "\n",
    "    # Filter only numerical columns for comparison\n",
    "    numeric_columns = df.select_dtypes(include=['float32', 'float64', 'int8', 'int16', 'int32', 'int64'])\n",
    "    dtype_after = numeric_columns.dtypes.copy()  # Copy of data types after compression\n",
    "    \n",
    "    # Create a comparison DataFrame\n",
    "    comparison_df = pd.DataFrame({'Before': dtype_before[numeric_columns.columns], 'After': dtype_after})\n",
    "    comparison_df['Size Reduction'] = ratio\n",
    "\n",
    "    return df, comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:46.990931Z",
     "iopub.status.busy": "2024-01-04T20:03:46.990301Z",
     "iopub.status.idle": "2024-01-04T20:03:47.029520Z",
     "shell.execute_reply": "2024-01-04T20:03:47.028535Z",
     "shell.execute_reply.started": "2024-01-04T20:03:46.990899Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate WordCloud\n",
    "\n",
    "def generate_wordcloud_subplot(df, label_value, subplot_position, max_words=1000, width=800, height=400, top_n = 10):\n",
    "    \"\"\"\n",
    "    Generate a word cloud for a specific label value and display it in a subplot.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing text data and labels.\n",
    "        label_value (int): The label value for which to generate the word cloud.\n",
    "        subplot_position (int): The position of the subplot where the word cloud will be displayed.\n",
    "        max_words (int, optional): Maximum number of words to include in the word cloud. Default is 1000.\n",
    "        width (int, optional): Width of the word cloud image. Default is 800.\n",
    "        height (int, optional): Height of the word cloud image. Default is 400.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Select the text subset for the specified label value\n",
    "    text_subset = df[df.generated == label_value].text\n",
    "\n",
    "    # Define stopwords to be excluded\n",
    "    stopwords = set(STOPWORDS)\n",
    "\n",
    "    # Create a WordCloud object with specified parameters\n",
    "    wc = WordCloud(max_words=max_words, width=width, height=height, stopwords=stopwords)\n",
    "\n",
    "    # Generate the word cloud from the selected text subset\n",
    "    wc.generate(\" \".join(text_subset))\n",
    "\n",
    "    # Create a subplot and display the word cloud\n",
    "    plt.subplot(subplot_position)\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "\n",
    "    # Set the title for the word cloud plot\n",
    "    title = f'WordCloud for Label {label_value} ({(\"Student\" if label_value == 0 else \"AI\")})'\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Count occurrences of words in the text subset\n",
    "    words_count = Counter(\" \".join(text_subset).split())\n",
    "    top_words = words_count.most_common(top_n)\n",
    "    bottom_words = words_count.most_common()[:-top_n-1:-1]  # Extract least common words\n",
    "\n",
    "    # Print the most common words\n",
    "    print(f\"Top {top_n} words for Label {label_value}:\")\n",
    "    for idx, (word, count) in enumerate(top_words, start=1):\n",
    "        print(f\"{idx}. {word}: {count} times\")\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "    # Print the least common words\n",
    "    print(f\"Least {top_n} words for Label {label_value}:\")\n",
    "    for idx, (word, count) in enumerate(bottom_words, start=1):\n",
    "        print(f\"{idx}. {word}: {count} times\")\n",
    "    print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='289C4E'>3. Import Data 📂<font><a class='anchor' id='data'></a> [↑](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 files which contains the following information:\n",
    "\n",
    "**test|train_essays.csv**\n",
    "\n",
    "- `id`: A unique identifier for each essay.\n",
    "\n",
    "- `prompt_id`: Identifies the prompt the essay was written in response to. \n",
    "\n",
    "- `text`: The essay text itself. \n",
    "\n",
    "- `generated`: Whether the essay was written by a student (0) or generated by an LLM (1). This field is the target and is not present in `test_essays.csv`.\n",
    "\n",
    "**train_prompts.csv**: Essays were written in response to information in these fields. \n",
    "\n",
    "- `prompt_id`: A unique identifier for each prompt. \n",
    "\n",
    "- `prompt_name`: The title of the prompt. \n",
    "\n",
    "- `instructions`: The instructions given to students. \n",
    "\n",
    "- `source_text`: The text of the article(s) the essays were written in response to, in Markdown format. Significant paragraphs are enumerated by a numeral preceding the paragraph on the same line, as in `0 Paragraph one.\\n\\n1 Paragraph two`. Essays sometimes refer to a paragraph by its numeral. Each article is preceded with its title in a heading, like `# Title`. When an author is indicated, their name will be given in the title after `by`. Not all articles have authors indicated. An article may have subheadings indicated like `## Subheading`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:47.033172Z",
     "iopub.status.busy": "2024-01-04T20:03:47.032476Z",
     "iopub.status.idle": "2024-01-04T20:03:47.045833Z",
     "shell.execute_reply": "2024-01-04T20:03:47.044944Z",
     "shell.execute_reply.started": "2024-01-04T20:03:47.033138Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Data\n",
    "\n",
    "data_path = '/kaggle/input/llm-detect-ai-generated-text/'\n",
    "\n",
    "for dirname, _, filenames in os.walk(data_path):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='289C4E'>3.1. Essays Datasets 🗞️<font><a class='anchor' id='essays'></a> [↑](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>3.1.1. Train Set 🚂<font><a class='anchor' id='essaystrain'></a> [↑](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we load the train dataset we see that it contains 1378 essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:47.048365Z",
     "iopub.status.busy": "2024-01-04T20:03:47.047618Z",
     "iopub.status.idle": "2024-01-04T20:03:47.193495Z",
     "shell.execute_reply": "2024-01-04T20:03:47.192607Z",
     "shell.execute_reply.started": "2024-01-04T20:03:47.048324Z"
    }
   },
   "outputs": [],
   "source": [
    "# Essays Train Dataset\n",
    "\n",
    "df_train_essays = pd.read_csv(data_path + \"train_essays.csv\")\n",
    "\n",
    "print(df_train_essays.info())\n",
    "df_train_essays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:47.195231Z",
     "iopub.status.busy": "2024-01-04T20:03:47.194884Z",
     "iopub.status.idle": "2024-01-04T20:03:47.221791Z",
     "shell.execute_reply": "2024-01-04T20:03:47.220679Z",
     "shell.execute_reply.started": "2024-01-04T20:03:47.195202Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compression\n",
    "\n",
    "# Compress dataframe\n",
    "df_train_essays, comparison_df = compress(df_train_essays, verbose=True)\n",
    "\n",
    "# Check compression\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:47.223425Z",
     "iopub.status.busy": "2024-01-04T20:03:47.222983Z",
     "iopub.status.idle": "2024-01-04T20:03:47.230525Z",
     "shell.execute_reply": "2024-01-04T20:03:47.229302Z",
     "shell.execute_reply.started": "2024-01-04T20:03:47.223396Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the first essay text\n",
    "\n",
    "df_train_essays.text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>3.1.2. Test Set 🧪<font><a class='anchor' id='essaystest'></a> [↑](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we load the test dataset we see that it only contains 3 essays. Additionally, the text only contains 12 characters. However the test set used for evaluation contains more essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:47.232412Z",
     "iopub.status.busy": "2024-01-04T20:03:47.231898Z",
     "iopub.status.idle": "2024-01-04T20:03:47.258728Z",
     "shell.execute_reply": "2024-01-04T20:03:47.257708Z",
     "shell.execute_reply.started": "2024-01-04T20:03:47.232366Z"
    }
   },
   "outputs": [],
   "source": [
    "# Essays Test Dataset\n",
    "\n",
    "df_test_essays = pd.read_csv(data_path + \"test_essays.csv\")\n",
    "\n",
    "print(df_test_essays.info())\n",
    "df_test_essays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:47.261118Z",
     "iopub.status.busy": "2024-01-04T20:03:47.260687Z",
     "iopub.status.idle": "2024-01-04T20:03:47.280270Z",
     "shell.execute_reply": "2024-01-04T20:03:47.278928Z",
     "shell.execute_reply.started": "2024-01-04T20:03:47.261079Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compression\n",
    "\n",
    "# Compress dataframe\n",
    "df_test_essays, comparison_df = compress(df_test_essays, verbose=True)\n",
    "\n",
    "# Check compression\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:47.286934Z",
     "iopub.status.busy": "2024-01-04T20:03:47.286153Z",
     "iopub.status.idle": "2024-01-04T20:03:47.294502Z",
     "shell.execute_reply": "2024-01-04T20:03:47.293044Z",
     "shell.execute_reply.started": "2024-01-04T20:03:47.286888Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the first essay text\n",
    "\n",
    "df_test_essays.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:47.298916Z",
     "iopub.status.busy": "2024-01-04T20:03:47.298285Z",
     "iopub.status.idle": "2024-01-04T20:03:47.308812Z",
     "shell.execute_reply": "2024-01-04T20:03:47.307582Z",
     "shell.execute_reply.started": "2024-01-04T20:03:47.298883Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking length of the essays\n",
    "\n",
    "df_test_essays[\"text\"].apply(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='289C4E'>3.2. Prompts Dataset 🗞️<font><a class='anchor' id='prompts'></a> [↑](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now explore the prompts dataset. We see that it only contains 2 entries, which means only 2 prompts or topics were created to produced several essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:47.310930Z",
     "iopub.status.busy": "2024-01-04T20:03:47.310393Z",
     "iopub.status.idle": "2024-01-04T20:03:47.337362Z",
     "shell.execute_reply": "2024-01-04T20:03:47.336217Z",
     "shell.execute_reply.started": "2024-01-04T20:03:47.310900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prompts Dataset\n",
    "\n",
    "df_train_prompts = pd.read_csv(data_path + \"train_prompts.csv\")\n",
    "print(df_train_prompts.info())\n",
    "\n",
    "df_train_prompts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:47.339362Z",
     "iopub.status.busy": "2024-01-04T20:03:47.338975Z",
     "iopub.status.idle": "2024-01-04T20:03:47.358904Z",
     "shell.execute_reply": "2024-01-04T20:03:47.356956Z",
     "shell.execute_reply.started": "2024-01-04T20:03:47.339331Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compression\n",
    "\n",
    "# Compress dataframe\n",
    "df_train_prompts, comparison_df = compress(df_train_prompts, verbose=True)\n",
    "\n",
    "# Check compression\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:47.360922Z",
     "iopub.status.busy": "2024-01-04T20:03:47.360605Z",
     "iopub.status.idle": "2024-01-04T20:03:47.367828Z",
     "shell.execute_reply": "2024-01-04T20:03:47.366744Z",
     "shell.execute_reply.started": "2024-01-04T20:03:47.360894Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the first instruction given to students\n",
    "\n",
    "df_train_prompts.instructions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:47.370398Z",
     "iopub.status.busy": "2024-01-04T20:03:47.369288Z",
     "iopub.status.idle": "2024-01-04T20:03:47.379428Z",
     "shell.execute_reply": "2024-01-04T20:03:47.378317Z",
     "shell.execute_reply.started": "2024-01-04T20:03:47.370365Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the first text\n",
    "\n",
    "df_train_prompts.source_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='289C4E'>4. Exploratory Data Analysis 📈<font><a class='anchor' id='eda'></a> [↑](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring the length and the content of the data set, we see that there are not much information to train a model and external data could be provided. Before that let's explore the distribution of the datasets and see how balanced or imbalanced they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='289C4E'>4.1. Distribution 📊<font><a class='anchor' id='distribution'></a> [↑](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>4.1.1 Essays 🚂<font><a class='anchor' id='distess'></a> [↑](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the prompts in the test set seems to be well balanced. On the other hand, there are only 3 texts generated by AI. Therefore, it will ne necessary to add additional data for the training to balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:47.380979Z",
     "iopub.status.busy": "2024-01-04T20:03:47.380682Z",
     "iopub.status.idle": "2024-01-04T20:03:47.745175Z",
     "shell.execute_reply": "2024-01-04T20:03:47.744082Z",
     "shell.execute_reply.started": "2024-01-04T20:03:47.380953Z"
    }
   },
   "outputs": [],
   "source": [
    "# Distribution of Prompts\n",
    "\n",
    "# Set Figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create the count plot\n",
    "ax = sns.countplot(data=df_train_essays, x=\"prompt_id\", palette=\"viridis\")\n",
    "\n",
    "# Mapping x-axis labels\n",
    "ax.set_xticklabels([\"Student\", \"AI\"])\n",
    "\n",
    "# Obtaining and setting the count values\n",
    "abs_values = df_train_essays['prompt_id'].value_counts().values\n",
    "ax.bar_label(container=ax.containers[0], labels=abs_values, fontsize=12)\n",
    "\n",
    "# Set title and labels with increased font sizes\n",
    "ax.set_title(\"Distribution of Prompt ID\", fontsize=16)\n",
    "ax.set_xlabel(\"Prompt ID\", fontsize=14)\n",
    "ax.set_ylabel(\"Count\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:47.746920Z",
     "iopub.status.busy": "2024-01-04T20:03:47.746576Z",
     "iopub.status.idle": "2024-01-04T20:03:48.072156Z",
     "shell.execute_reply": "2024-01-04T20:03:48.071025Z",
     "shell.execute_reply.started": "2024-01-04T20:03:47.746890Z"
    }
   },
   "outputs": [],
   "source": [
    "# Distribution of Generated Text\n",
    "\n",
    "# Set Figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create the count plot\n",
    "ax = sns.countplot(data=df_train_essays, x=\"generated\", palette=\"viridis\")\n",
    "\n",
    "# Mapping x-axis labels\n",
    "ax.set_xticklabels([\"Student\", \"AI\"])\n",
    "\n",
    "# Obtaining and setting the count values\n",
    "abs_values = df_train_essays['generated'].value_counts().values\n",
    "ax.bar_label(container=ax.containers[0], labels=abs_values, fontsize=12)\n",
    "\n",
    "# Set title and labels with increased font sizes\n",
    "ax.set_title(\"Distribution of Generated Text\", fontsize=16)\n",
    "ax.set_xlabel(\"Generated Text\", fontsize=14)\n",
    "ax.set_ylabel(\"Count\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='289C4E'>5. External Essays 📤<font><a class='anchor' id='distext'></a> [↑](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the train set is imbalanced, we have the possibility to import external data. In our case we will use the [DAIGT](https://www.kaggle.com/datasets/thedrcat/daigt-proper-train-dataset/) which contains the original `train_essays.csv` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='289C4E'>5.1. DAIGT Essay 1️⃣<font><a class='anchor' id='daigt'></a> [↑](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:48.074005Z",
     "iopub.status.busy": "2024-01-04T20:03:48.073553Z",
     "iopub.status.idle": "2024-01-04T20:03:50.340018Z",
     "shell.execute_reply": "2024-01-04T20:03:50.338566Z",
     "shell.execute_reply.started": "2024-01-04T20:03:48.073973Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import external dataset\n",
    "\n",
    "ext1 = pd.read_csv('/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:50.341692Z",
     "iopub.status.busy": "2024-01-04T20:03:50.341351Z",
     "iopub.status.idle": "2024-01-04T20:03:50.353841Z",
     "shell.execute_reply": "2024-01-04T20:03:50.352823Z",
     "shell.execute_reply.started": "2024-01-04T20:03:50.341663Z"
    }
   },
   "outputs": [],
   "source": [
    "ext1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:50.355062Z",
     "iopub.status.busy": "2024-01-04T20:03:50.354764Z",
     "iopub.status.idle": "2024-01-04T20:03:50.676081Z",
     "shell.execute_reply": "2024-01-04T20:03:50.674940Z",
     "shell.execute_reply.started": "2024-01-04T20:03:50.355035Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check duplicates\n",
    "\n",
    "# Get the number of rows with duplicates\n",
    "duplicates = ext1.duplicated().sum()\n",
    "\n",
    "# Print the number of rows before and after\n",
    "print(f\"Number of rows with duplicates: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:50.677513Z",
     "iopub.status.busy": "2024-01-04T20:03:50.677189Z",
     "iopub.status.idle": "2024-01-04T20:03:50.699261Z",
     "shell.execute_reply": "2024-01-04T20:03:50.698153Z",
     "shell.execute_reply.started": "2024-01-04T20:03:50.677485Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compression\n",
    "\n",
    "# Compress dataframe\n",
    "ext1, comparison_df = compress(ext1, verbose=True)\n",
    "\n",
    "# Check compression\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains several columns that will not be necessary for our specific problem. Therefore we will remove them and rename the `label` column to `generated`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:50.701432Z",
     "iopub.status.busy": "2024-01-04T20:03:50.700985Z",
     "iopub.status.idle": "2024-01-04T20:03:50.715647Z",
     "shell.execute_reply": "2024-01-04T20:03:50.714504Z",
     "shell.execute_reply.started": "2024-01-04T20:03:50.701392Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop Columns\n",
    "\n",
    "ext1.drop([\"RDizzl3_seven\", \"prompt_name\", \"source\", \"prompt_name\"], inplace=True, axis=1)\n",
    "\n",
    "ext1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:50.717322Z",
     "iopub.status.busy": "2024-01-04T20:03:50.716947Z",
     "iopub.status.idle": "2024-01-04T20:03:50.730350Z",
     "shell.execute_reply": "2024-01-04T20:03:50.729170Z",
     "shell.execute_reply.started": "2024-01-04T20:03:50.717292Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rename label column\n",
    "\n",
    "ext1.rename(columns = {\"label\":\"generated\"}, inplace=True)\n",
    "\n",
    "ext1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='289C4E'>5.2. Distribution ⚖️<font><a class='anchor' id='dist'></a> [↑](#top)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the distribution of the labels. We see that the dataset is imbalanced. However, in order to set a RNN baseline model, we will use the dataset as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:50.731999Z",
     "iopub.status.busy": "2024-01-04T20:03:50.731645Z",
     "iopub.status.idle": "2024-01-04T20:03:51.048052Z",
     "shell.execute_reply": "2024-01-04T20:03:51.046819Z",
     "shell.execute_reply.started": "2024-01-04T20:03:50.731971Z"
    }
   },
   "outputs": [],
   "source": [
    "# Distribution of Generated Text in the External Dataset\n",
    "\n",
    "# Set Figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create the count plot\n",
    "ax = sns.countplot(data=ext1, x=\"generated\", palette=\"viridis\")\n",
    "\n",
    "# Mapping x-axis labels\n",
    "ax.set_xticklabels([\"Student\", \"AI\"])\n",
    "\n",
    "# Obtaining and setting the count values\n",
    "abs_values = ext1['generated'].value_counts().values\n",
    "ax.bar_label(container=ax.containers[0], labels=abs_values, fontsize=12)\n",
    "\n",
    "# Set title and labels with increased font sizes\n",
    "ax.set_title(\"Distribution of Generated Text\", fontsize=16)\n",
    "ax.set_xlabel(\"Generated Text\", fontsize=14)\n",
    "ax.set_ylabel(\"Count\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='289C4E'>5.3. Wordcloud 🤼‍<font><a class='anchor' id='wc'></a> [↑](#top)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will go deeper in the analysis. We will plot the wordcloud an the list of most and less common words.\n",
    "\n",
    "We see that the most common words are tipically stopwords, which we could remove. Additionally some numbers appear, which are also not relevant and can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:03:51.051138Z",
     "iopub.status.busy": "2024-01-04T20:03:51.050200Z",
     "iopub.status.idle": "2024-01-04T20:05:05.267909Z",
     "shell.execute_reply": "2024-01-04T20:05:05.266482Z",
     "shell.execute_reply.started": "2024-01-04T20:03:51.051101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot WordCloud\n",
    "\n",
    "# Create a 1x2 grid of subplots\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Generate WordCloud for label_value = 1 (subplot 1)\n",
    "generate_wordcloud_subplot(ext1, label_value=1, subplot_position=121, top_n = 10)\n",
    "\n",
    "# Generate WordCloud for label_value = 0 (subplot 2)\n",
    "generate_wordcloud_subplot(ext1, label_value=0, subplot_position=122, top_n = 10)\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing between subplots\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='289C4E'>5.4. Preprocessing 🏋️<font><a class='anchor' id='prepro'></a> [↑](#top)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several preprocessing steps that we will perform:\n",
    "\n",
    "* Clean text: lower text, removal of punctiation, extra spaces, whitespaces and numbers\n",
    "\n",
    "* Stopwords removal\n",
    "\n",
    "This will help to feed the model with mode relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:05:05.269875Z",
     "iopub.status.busy": "2024-01-04T20:05:05.269534Z",
     "iopub.status.idle": "2024-01-04T20:05:25.566508Z",
     "shell.execute_reply": "2024-01-04T20:05:25.565310Z",
     "shell.execute_reply.started": "2024-01-04T20:05:05.269847Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clean Text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Replace actual newline and carriage return characters with whitespace\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\r\", \" \")\n",
    "    \n",
    "    # Drop punctuation\n",
    "    text = re.sub(r\"\\p{P}\", \" \", text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    # Remove leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Lower text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to the 'text' column in the DataFrame\n",
    "ext1['text'] = ext1['text'].apply(clean_text)\n",
    "\n",
    "# Change contractions\n",
    "contractions = {\n",
    "    r'\\b(can\\'t)\\b': 'cannot',\n",
    "    r'\\b(don\\'t)\\b': 'do not',\n",
    "    r'\\b(won\\'t)\\b': 'will not',\n",
    "}\n",
    "\n",
    "# Iterate through contractions and apply replacements to the entire DataFrame column\n",
    "for pattern, replacement in contractions.items():\n",
    "    ext1['text'] = ext1['text'].apply(lambda x: re.sub(pattern, replacement, x, flags=re.IGNORECASE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:05:25.568576Z",
     "iopub.status.busy": "2024-01-04T20:05:25.568211Z",
     "iopub.status.idle": "2024-01-04T20:05:25.584666Z",
     "shell.execute_reply": "2024-01-04T20:05:25.583445Z",
     "shell.execute_reply.started": "2024-01-04T20:05:25.568547Z"
    }
   },
   "outputs": [],
   "source": [
    "# As NLTK is not working in Kaggle. We set the stopwords list\n",
    "\n",
    "stopword_list = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:05:25.592415Z",
     "iopub.status.busy": "2024-01-04T20:05:25.591977Z",
     "iopub.status.idle": "2024-01-04T20:06:04.718564Z",
     "shell.execute_reply": "2024-01-04T20:06:04.717404Z",
     "shell.execute_reply.started": "2024-01-04T20:05:25.592384Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove Stopwords\n",
    "\n",
    "def remove_custom_stopwords(sentence):\n",
    "    words = sentence.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stopword_list]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "ext1['text'] = ext1['text'].apply(remove_custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:06:04.721178Z",
     "iopub.status.busy": "2024-01-04T20:06:04.720819Z",
     "iopub.status.idle": "2024-01-04T20:06:04.900085Z",
     "shell.execute_reply": "2024-01-04T20:06:04.898985Z",
     "shell.execute_reply.started": "2024-01-04T20:06:04.721148Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check duplicates\n",
    "\n",
    "# Get the number of rows with duplicates\n",
    "duplicates = ext1.duplicated().sum()\n",
    "\n",
    "# Print the number of rows before and after\n",
    "print(f\"Number of rows with duplicates: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing if we plot the stopwords again, we see that the list of works looks more relevant and this will improve the results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:06:04.901994Z",
     "iopub.status.busy": "2024-01-04T20:06:04.901561Z",
     "iopub.status.idle": "2024-01-04T20:07:16.599818Z",
     "shell.execute_reply": "2024-01-04T20:07:16.598575Z",
     "shell.execute_reply.started": "2024-01-04T20:06:04.901954Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot WordCloud\n",
    "\n",
    "# Create a 1x2 grid of subplots\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Generate WordCloud for label_value = 1 (subplot 1)\n",
    "generate_wordcloud_subplot(ext1, label_value=1, subplot_position=121, top_n = 10)\n",
    "\n",
    "# Generate WordCloud for label_value = 0 (subplot 2)\n",
    "generate_wordcloud_subplot(ext1, label_value=0, subplot_position=122, top_n = 10)\n",
    "\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing between subplots\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='289C4E'>6. Modeling 💪<font><a class='anchor' id='modeling'></a> [↑](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset clean let's perform our training with an RNN model. First, let's make a copy of the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:07:16.601967Z",
     "iopub.status.busy": "2024-01-04T20:07:16.601471Z",
     "iopub.status.idle": "2024-01-04T20:07:16.612451Z",
     "shell.execute_reply": "2024-01-04T20:07:16.611642Z",
     "shell.execute_reply.started": "2024-01-04T20:07:16.601929Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copy the final_df as df_model\n",
    "\n",
    "df_model = ext1.copy()\n",
    "\n",
    "df_model.generated.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='289C4E'>6.1. Train/Test Split 🪓<font><a class='anchor' id='split'></a> [↑](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will split the dataset in train, test and validation. We will do it after shuffling it, so that we get a good label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:07:16.614548Z",
     "iopub.status.busy": "2024-01-04T20:07:16.613751Z",
     "iopub.status.idle": "2024-01-04T20:07:16.636023Z",
     "shell.execute_reply": "2024-01-04T20:07:16.634601Z",
     "shell.execute_reply.started": "2024-01-04T20:07:16.614492Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a shuffled df for a good labels distribution\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "random_seed = 42\n",
    "\n",
    "print(\"Before shuffling:\", df_model.shape)\n",
    "\n",
    "# Shuffle the DataFrame with the specified random seed\n",
    "shuffled_df = df_model.sample(frac=1, random_state=random_seed)\n",
    "\n",
    "print(\"After shuffling:\", df_model.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:07:16.637943Z",
     "iopub.status.busy": "2024-01-04T20:07:16.637622Z",
     "iopub.status.idle": "2024-01-04T20:07:16.659557Z",
     "shell.execute_reply": "2024-01-04T20:07:16.658336Z",
     "shell.execute_reply.started": "2024-01-04T20:07:16.637915Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a train/val/test split \n",
    "X = shuffled_df[\"text\"]\n",
    "y = shuffled_df[\"generated\"]\n",
    "\n",
    "\n",
    "# Split the data into train, validation, and test sets (80% train, 15% validation, 15% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=random_seed)\n",
    "\n",
    "# Display the shapes of the train, validation, and test sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_validation shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_validation shape:\", y_val.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we see the distributions of the labels, we can appreciate a similar distribution in each set, which will help to a better performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:07:16.662004Z",
     "iopub.status.busy": "2024-01-04T20:07:16.661397Z",
     "iopub.status.idle": "2024-01-04T20:07:17.199415Z",
     "shell.execute_reply": "2024-01-04T20:07:17.198218Z",
     "shell.execute_reply.started": "2024-01-04T20:07:16.661960Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get label counts for train, validation, and test data\n",
    "train_label_counts = y_train.value_counts()\n",
    "val_label_counts = y_val.value_counts()\n",
    "test_label_counts = y_test.value_counts()\n",
    "\n",
    "# Define custom labels for visualization\n",
    "custom_labels = {0: 'Student', 1: 'AI'}\n",
    "\n",
    "# Replace labels for visualization purposes\n",
    "train_labels_visual = train_label_counts.rename(custom_labels)\n",
    "val_labels_visual = val_label_counts.rename(custom_labels)\n",
    "test_labels_visual = test_label_counts.rename(custom_labels)\n",
    "\n",
    "# Define custom colors for each label\n",
    "label_colors = {'Student': '#33FF57', 'AI': '#FF5733'}\n",
    "\n",
    "# Create subplots with 1 row and 3 columns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Subplot 1: Train data distribution\n",
    "wedges, texts, autotexts = axes[0].pie(train_labels_visual, labels=train_labels_visual.index, autopct='%1.1f%%', colors=[label_colors[label] for label in train_labels_visual.index])\n",
    "axes[0].set_title('Train Data Distribution')\n",
    "\n",
    "# Subplot 2: Validation data distribution\n",
    "wedges, texts, autotexts = axes[1].pie(val_labels_visual, labels=val_labels_visual.index, autopct='%1.1f%%', colors=[label_colors[label] for label in val_labels_visual.index])\n",
    "axes[1].set_title('Validation Data Distribution')\n",
    "\n",
    "# Subplot 3: Test data distribution\n",
    "wedges, texts, autotexts = axes[2].pie(test_labels_visual, labels=test_labels_visual.index, autopct='%1.1f%%', colors=[label_colors[label] for label in test_labels_visual.index])\n",
    "axes[2].set_title('Test Data Distribution')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='289C4E'>6.2. Vectorization ↗️<font><a class='anchor' id='vector'></a> [↑](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the vestorization we will use the **TextVectorization** layer from TensorFlow. This process prepares text data for input into a machine learning model by converting text into numerical representations that the model can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:07:17.201611Z",
     "iopub.status.busy": "2024-01-04T20:07:17.201284Z",
     "iopub.status.idle": "2024-01-04T20:07:22.173342Z",
     "shell.execute_reply": "2024-01-04T20:07:22.172031Z",
     "shell.execute_reply.started": "2024-01-04T20:07:17.201584Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the max vocaulary size\n",
    "\n",
    "text_vectorizer = TextVectorization(split=\"whitespace\",\n",
    "                                    output_mode=\"int\")\n",
    "\n",
    "# Fit the text vectorizer\n",
    "text_vectorizer.adapt(X)\n",
    "\n",
    "# Get the number of unique tokens in the vocabulary\n",
    "vocab_size = len(text_vectorizer.get_vocabulary())\n",
    "\n",
    "# Print the vocabulary size\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:07:22.175176Z",
     "iopub.status.busy": "2024-01-04T20:07:22.174820Z",
     "iopub.status.idle": "2024-01-04T20:07:25.839233Z",
     "shell.execute_reply": "2024-01-04T20:07:25.838139Z",
     "shell.execute_reply.started": "2024-01-04T20:07:22.175145Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup text vectorization with custom variables\n",
    "\n",
    "# Set the maximum vocabulary size\n",
    "# max_vocab_size = 10000\n",
    "max_vocab_size = vocab_size \n",
    "\n",
    "# Calculate the maximum sequence length based on the average number of tokens in training data\n",
    "average_tokens_per_sequence = round(sum([len(text.split()) for text in X_train]) / len(X_train))\n",
    "\n",
    "# Create and configure the TextVectorization layer\n",
    "text_vectorizer = TextVectorization(\n",
    "    max_tokens=max_vocab_size,\n",
    "#     ngrams=(3,5),\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=average_tokens_per_sequence,\n",
    "    pad_to_max_tokens=True\n",
    ")\n",
    "\n",
    "# Adapt the TextVectorization layer to the training text\n",
    "if len(X_train) > 0:\n",
    "    text_vectorizer.adapt(X_train)\n",
    "else:\n",
    "    print(\"Warning: X_train is empty, adaptation skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='289C4E'>6.3. Embedding 📦<font><a class='anchor' id='emb'></a> [↑](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After vectorization we will create an **Embedding** layer. This Embedding layer converts input text data (represented as indices or sequences of integers) into dense vectors of fixed size (output_dim) in the embedding space. These dense vectors serve as the input for subsequent layers in the neural network model, allowing the model to learn meaningful representations of words based on their contexts within the input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:07:25.841531Z",
     "iopub.status.busy": "2024-01-04T20:07:25.840941Z",
     "iopub.status.idle": "2024-01-04T20:07:25.856567Z",
     "shell.execute_reply": "2024-01-04T20:07:25.855310Z",
     "shell.execute_reply.started": "2024-01-04T20:07:25.841489Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "embedding = layers.Embedding(input_dim=max_vocab_size,          \n",
    "                             output_dim=128,\n",
    "                             embeddings_initializer=\"uniform\",\n",
    "                             input_length=average_tokens_per_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='289C4E'>6.4. RNN Model 🎢<font><a class='anchor' id='model'></a> [↑](#top) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline model is set up for binary classification tasks where the input is text data, and the objective is to predict a binary outcome (Student or AI generated). The text is tokenized, embedded, and then processed through a simple neural network architecture for classification. We have added som callbacks for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>6.4.1. Fitting 🏃<font><a class='anchor' id='fit'></a> [↑](#top) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:07:25.858677Z",
     "iopub.status.busy": "2024-01-04T20:07:25.858198Z",
     "iopub.status.idle": "2024-01-04T20:07:26.197760Z",
     "shell.execute_reply": "2024-01-04T20:07:26.196647Z",
     "shell.execute_reply.started": "2024-01-04T20:07:25.858633Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "keras_model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile model\n",
    "keras_model.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Get a summary of the model\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:07:26.199769Z",
     "iopub.status.busy": "2024-01-04T20:07:26.199435Z",
     "iopub.status.idle": "2024-01-04T20:44:58.128722Z",
     "shell.execute_reply": "2024-01-04T20:44:58.127320Z",
     "shell.execute_reply.started": "2024-01-04T20:07:26.199740Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "\n",
    "callbacks = [ModelCheckpoint(filepath='keras_model', save_best_only=True, save_format='tf'),\n",
    "             EarlyStopping(patience=7, monitor='val_loss', restore_best_weights = True),\n",
    "             ReduceLROnPlateau(factor=0.2, patience=5, monitor='val_loss'),\n",
    "             CSVLogger('keras_training_log.csv')]\n",
    "\n",
    "\n",
    "keras_model_history = keras_model.fit(X_train,                                      \n",
    "                                      y_train,\n",
    "                                      epochs=100,\n",
    "                                      validation_data=(X_val, y_val),\n",
    "                                      callbacks=callbacks,\n",
    "                                     # batch_size=32\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>6.4.2. Learning Curves ↘️<font><a class='anchor' id='lc'></a> [↑](#top) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This **learning curves** helps in assessing the model's training progress and observing the trend of both training and validation losses over epochs. We can appreciate that the learning curves are shwoing a good trend and low values. However, the fit very fast and show some overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:44:58.131013Z",
     "iopub.status.busy": "2024-01-04T20:44:58.130572Z",
     "iopub.status.idle": "2024-01-04T20:44:58.600835Z",
     "shell.execute_reply": "2024-01-04T20:44:58.599595Z",
     "shell.execute_reply.started": "2024-01-04T20:44:58.130972Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(keras_model_history.history['loss'], label='Training Loss')\n",
    "plt.plot(keras_model_history.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "\n",
    "# Set y-axis lower limit to 0.5\n",
    "plt.ylim(top=0.5)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>6.4.3. Confusion Matrix 🧮<font><a class='anchor' id='cm'></a> [↑](#top) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **confusion matrix** aids in understanding the model's accuracy in predicting each class and the misclassifications made by the model on the test dataset (X_test). In our case we get excellent results with low amount of FP/FN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:44:58.602968Z",
     "iopub.status.busy": "2024-01-04T20:44:58.602534Z",
     "iopub.status.idle": "2024-01-04T20:45:00.100950Z",
     "shell.execute_reply": "2024-01-04T20:45:00.099743Z",
     "shell.execute_reply.started": "2024-01-04T20:44:58.602926Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on input data (X_test) in form of probabilities\n",
    "\n",
    "keras_probabilities = keras_model.predict(X_test)\n",
    "keras_probabilities[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:45:00.103525Z",
     "iopub.status.busy": "2024-01-04T20:45:00.103067Z",
     "iopub.status.idle": "2024-01-04T20:45:00.118561Z",
     "shell.execute_reply": "2024-01-04T20:45:00.117319Z",
     "shell.execute_reply.started": "2024-01-04T20:45:00.103483Z"
    }
   },
   "outputs": [],
   "source": [
    "# Turn prediction probabilities into single-dimension tensor of floats\n",
    "\n",
    "# squeeze removes single dimensions\n",
    "keras_prediction = tf.squeeze(tf.round(keras_probabilities))\n",
    "keras_prediction[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:45:00.120495Z",
     "iopub.status.busy": "2024-01-04T20:45:00.120117Z",
     "iopub.status.idle": "2024-01-04T20:45:00.462042Z",
     "shell.execute_reply": "2024-01-04T20:45:00.460919Z",
     "shell.execute_reply.started": "2024-01-04T20:45:00.120464Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "\n",
    "keras_cm = confusion_matrix(y_test, keras_prediction)\n",
    "keras_cm_plot =ConfusionMatrixDisplay(confusion_matrix=keras_cm)\n",
    "\n",
    "keras_cm_plot.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>6.4.4. Classification Report 🏅<font><a class='anchor' id='cr'></a> [↑](#top) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This **classification report** evaluates the model's performance using classification metrics and provides insights into its precision, recall, and f1-score for each class, helping to assess how well the model performs in classifying instances in the test dataset (X_test). In our case we get excellent results in all the parameters (precision, recall, f1-score and accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:45:00.463751Z",
     "iopub.status.busy": "2024-01-04T20:45:00.463401Z",
     "iopub.status.idle": "2024-01-04T20:45:01.431207Z",
     "shell.execute_reply": "2024-01-04T20:45:01.430057Z",
     "shell.execute_reply.started": "2024-01-04T20:45:00.463721Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predictions from the model on the test set\n",
    "y_pred = keras_model.predict(X_test)\n",
    "\n",
    "# Converting probabilities to classes (assuming a threshold of 0.5)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>6.4.5. AUC - ROC Curve 📐<font><a class='anchor' id='roc'></a> [↑](#top) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ROC curve**, illustrates the trade-off between the true positive rate and false positive rate across different thresholds. The AUC value quantifies the overall performance of the model in distinguishing between the positive and negative classes, with a higher AUC indicating better performance. In our case, again the results are excellent. Note that this is the metric use to evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:45:01.432963Z",
     "iopub.status.busy": "2024-01-04T20:45:01.432636Z",
     "iopub.status.idle": "2024-01-04T20:45:01.855927Z",
     "shell.execute_reply": "2024-01-04T20:45:01.854719Z",
     "shell.execute_reply.started": "2024-01-04T20:45:01.432934Z"
    }
   },
   "outputs": [],
   "source": [
    "# AUC -  ROC Curve\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "# Calculate AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "\n",
    "# Set labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "# Set title and legend\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>6.4.6. Precission-Recall Curve 📞<font><a class='anchor' id='prc'></a> [↑](#top) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **precision-recall curve** illustrates the trade-off between precision and recall for different probability thresholds. A higher AUC value indicates better performance of the model in terms of both precision and recall for different classification thresholds. This curve provides valuable insights into the model's performance, especially in scenarios with imbalanced class distributions. In our case, again the results are excellent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:45:01.857961Z",
     "iopub.status.busy": "2024-01-04T20:45:01.857392Z",
     "iopub.status.idle": "2024-01-04T20:45:02.215803Z",
     "shell.execute_reply": "2024-01-04T20:45:02.214496Z",
     "shell.execute_reply.started": "2024-01-04T20:45:01.857927Z"
    }
   },
   "outputs": [],
   "source": [
    "# Precission-recall curve\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, keras_probabilities)\n",
    "\n",
    "# Calculate AUC for precision-recall curve\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='orange', lw=2, label='Precision-Recall curve (AUC = {:.2f})'.format(pr_auc))\n",
    "\n",
    "# Set labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "# Set title and legend\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower left')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='289C4E'>7. Submission 📩<font><a class='anchor' id='sub'></a> [↑](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's submit our baseline model predictions!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:45:02.218501Z",
     "iopub.status.busy": "2024-01-04T20:45:02.217663Z",
     "iopub.status.idle": "2024-01-04T20:45:02.300732Z",
     "shell.execute_reply": "2024-01-04T20:45:02.299718Z",
     "shell.execute_reply.started": "2024-01-04T20:45:02.218465Z"
    }
   },
   "outputs": [],
   "source": [
    "test_prediction = keras_model.predict(df_test_essays[\"text\"])\n",
    "test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T20:45:02.303424Z",
     "iopub.status.busy": "2024-01-04T20:45:02.302430Z",
     "iopub.status.idle": "2024-01-04T20:45:02.319436Z",
     "shell.execute_reply": "2024-01-04T20:45:02.318211Z",
     "shell.execute_reply.started": "2024-01-04T20:45:02.303388Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the submission\n",
    "submission_df = df_test_essays[[\"id\"]].copy()\n",
    "\n",
    "# Add the formatted predictions to the submission DataFrame\n",
    "submission_df[\"generated\"] = test_prediction.squeeze()\n",
    "\n",
    "# Save Submission\n",
    "submission_df.to_csv('submission.csv',index=False)\n",
    "\n",
    "# Display the first 2 rows of the submission DataFrame\n",
    "submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 6888007,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "datasetId": 4005256,
     "sourceId": 6977472,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 4675,
     "sourceId": 5902,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 4678,
     "sourceId": 5905,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 4686,
     "sourceId": 5913,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 4689,
     "sourceId": 5916,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 4723,
     "sourceId": 5950,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
